import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


student_data=pd.read_csv("data/student_por (1).csv")
df_student=student_data
df_student.head()


df_student.info()


df_student.plot(kind="hist");


df_student.hist(figsize=(20,16));


sns.histplot(df_student['G3'], kde=True)
plt.title("Distribution of Final Grades (G3)")
plt.show()


df_student.describe()


final_grade=df_student['G3']
at_risk = [1 if G3 < 7.5 else 0 for G3 in final_grade]
df_student['at_risk'] = at_risk





df_student.groupby("internet")['G3'].count()


# Who is at risk ??
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

sns.countplot(x='sex', hue='at_risk', data=df_student, ax=axes[0])
axes[0].set_title("At Risk by Sex")

sns.countplot(x='internet', hue='at_risk', data=df_student, ax=axes[1])
axes[1].set_title("At Risk by Internet Access")

plt.tight_layout()
plt.show()


#gender counting by %
df_student['sex'].value_counts(normalize=True)



#looking into mjob 
sns.countplot(x="Mjob",hue="at_risk",data=df_student)
plt.show()


#looking into Fjob  
sns.countplot(x="Fjob",hue="at_risk",data=df_student)


df_student['Fjob'].value_counts().plot(kind='bar')
plt.show()


df_student['reason'].value_counts().plot(kind='bar')
plt.show()


second_period_grade=df_student["G2"]
studytime=df_student["studytime"]
first_period_grade=df_student['G1']
plt.scatter(studytime,final_grade)
#plt.plot(studytime,second_period_grade,'g',label="second_period_grade(G2)")
#plt.plot(studytime,first_period_grade,'r',label="first_period_grade(G1)")
plt.xlabel("Study Time")
plt.ylabel("Grades")
plt.title("Grade vs Study Time")

plt.grid(True)
plt.show()



#grades corellation
sns.heatmap(df_student[['G1', 'G2', 'G3']].corr(), annot=True)


#risk due to class absence
sns.boxplot(x='at_risk', y='absences', data=df_student)
plt.show()


#low grade is  caused  by age???
sns.boxplot(x='at_risk', y='age', data=df_student)
plt.show()


#let's see  study habit
sns.scatterplot(x='studytime', y='G3', hue='at_risk', data=df_student)
plt.ylabel("final_grade(G3)")
plt.title("Grades vs studytime")
plt.show()



sns.scatterplot(x='studytime', y='failures', hue='at_risk', data=df_student)
plt.show()


sns.scatterplot(x='studytime', y='goout', hue='at_risk', data=df_student)
plt.show()



#notice--famlrel,fed,med are already encoded into ordinal encoder
#let's use one hot encoder for mjob,fjob,use pd.get_dummies



df_student.head()


sns.barplot(x="address",y="G3",data=df_student)


#df_student = pd.get_dummies(df_student, columns=['Mjob', 'Fjob', 'reason'])


df_student.head()


df_student.select_dtypes(exclude="object").corr()





#B .Feature creation.......
df_student.select_dtypes(include="object").columns


#splitting data for training and test
from sklearn.model_selection import train_test_split
#drop column that irrelevant 
attr_drop=["school","guardian","reason",'Medu','Fedu'
        
           ,"G3"]
            
X=df_student.drop(attr_drop,axis=1)#dropping target from trainset
y=df_student["G3"]#target 
#trainset and testset .we are using 20% for test rest is for training
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)


from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder

#extract categorical features and numericals
cat_features=list(X_train.select_dtypes(include="object").columns)
num_features=list(X_train.select_dtypes(exclude="object").columns)


type(num_features)


#preparing data
data_preprocessor=ColumnTransformer(transformers=[("num",StandardScaler(),num_features),
                                                  ("cat",OneHotEncoder(handle_unknown='ignore'),cat_features)
                                                 
                                                 ])
data_preprocessor








#first we try OLR MODEL
from sklearn.linear_model import LinearRegression

linear_reg_model=Pipeline(steps=[("preprocessor",data_preprocessor),
                                 ("linear_model",LinearRegression())])




linear_reg_model


linear_reg_model.fit(X_train,y_train)# train model








y_train_predict=linear_reg_model.predict(X_train)


from sklearn.metrics import mean_squared_error
mse=mean_squared_error(y_train,y_train_predict)
mse=np.sqrt(mse)
mse


residual=y_train-y_train_predict
residual.hist()
plt.title("Residual of the linear model")
plt.show()


#evaluating by cross_val_score
from sklearn.model_selection import cross_val_score
score_mse=cross_val_score(linear_reg_model,X_train,y_train, scoring="neg_mean_squared_error",cv=5)



def display_scores(scores):     
    print("Scores:", scores) 
    print("Mean:", scores.mean())
    print("Standard deviation:", scores.std())


display_scores(score_mse)


#random_forest
from sklearn.ensemble import RandomForestRegressor
# randomforest pipeline
model_pipeline = Pipeline(steps=[
    ('preprocessor', data_preprocessor),
    ('regressor', RandomForestRegressor())])
model_pipeline 
    


# Train
model_pipeline.fit(X_train, y_train)







mse_randomforest=mean_squared_error(y_train,model_pipeline.predict(X_train))
mse_randomforest=np.sqrt(mse_randomforest)
mse_randomforest


score_randomforest=cross_val_score(model_pipeline,X_train,y_train, 
                                   scoring="neg_mean_squared_error",cv=5)


display_scores(score_randomforest)


#fine_tunning forestregression
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
#random_forest_model_pipeline
tunned_forest_reg=Pipeline(steps=[
    ('preprocessor', data_preprocessor),
    ('regressor', RandomForestRegressor(random_state=42))])
# Hyperparameter grid for GridSearchCV
param_grid = {
    'regressor__n_estimators': [100, 200, 300],  # Number of trees
    'regressor__max_depth': [10, 20, None],       # Depth of trees
    'regressor__min_samples_split': [2, 5, 10],   # Min samples to split a node
    'regressor__min_samples_leaf': [1, 2, 4],     # Min samples at a leaf node
    'regressor__max_features': ['sqrt', 'log2']   # Features to consider for splits
}
# Initialize GridSearchCV
grid_search = GridSearchCV(
    estimator=tunned_forest_reg,
    param_grid=param_grid,
    cv=5,                   # 5-fold cross-validation
    scoring='neg_mean_squared_error',  # Optimize for RMSE
    verbose=2,              # Print progress
    n_jobs=-1               # Use all CPU cores
)


##param_grid = [
## {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},
## {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},
## ]
##grid_search = GridSearchCV(forest_reg, param_grid, cv=5,
##     scoring='neg_mean_squared_error',
 #    return_train_score=True)



# Fit the grid search to the training data
grid_search.fit(X_train, y_train)


# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_predictor_model = grid_search.best_estimator_


# Predict on test data
y_pred = best_predictor_model.predict(X_test)

# Calculate metrics
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"RMSE: {rmse:.2f}")
print(f"RÂ²: {r2:.2f}")


residual_testset=y_test-y_pred
residual_testset.hist()
plt.title("Residual on the testset")
plt.show()


# Extract feature importances from the best model
importances = best_predictor_model.named_steps['regressor'].feature_importances_

# Get feature names after preprocessing
feature_names = (
    num_features + 
    best_predictor_model.named_steps['preprocessor'].named_transformers_['cat']
    .get_feature_names_out(cat_features).tolist()
)

# Create a DataFrame for visualization
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

print(importance_df.head(10))  # Top 10 important features







y=importance_df['Feature'].head(10)
plt.barh(y,width=importance_df['Importance'].head(10))
plt.ylabel("features")
plt.xlabel("Importance")
plt.title("Important features")
plt.show()


from joblib import dump

# Save the model to a file for later use 
dump(best_predictor_model, 'student_performance_predictor_model.joblib')


#let's check
from joblib import load

# Load the model
loaded_model = load('student_performance_predictor_model.joblib')

# Example prediction
sample_data = X_test.iloc[0:1]  # Use a DataFrame row from your dataset
prediction = loaded_model.predict(sample_data)
print(f"Predicted Final Grade: {prediction[0]:.2f}")


#selecting important  features 
features = ['G1', 'G2', 'failures', 'absences', 'higher', 'studytime', 'age', 'Dalc', 'goout']
target = 'at_risk'
X=df_student.drop(target,axis=1)
y=df_student[target]


#feature splitting
X_trainset, X_testset, y_trainset, y_testset = train_test_split(X, y, test_size=0.2, stratify=y)


#encoding features
#extract categorical features and numericals
cat_features=list(X_trainset.select_dtypes(include="object").columns)
num_features=list(X_trainset.select_dtypes(exclude="object").columns)
#encoding features
preprocessor=ColumnTransformer(transformers=[("num",StandardScaler(),num_features),
                                                  ("cat",OneHotEncoder(handle_unknown='ignore'),cat_features)
                                                 
                                                 ])
preprocessor


from sklearn.ensemble import RandomForestClassifier

classif_pipeline=Pipeline(steps=[("preprocessor",preprocessor),
                                 ("classification_model",RandomForestClassifier())])


classif_pipeline


classif_pipeline.fit(X_trainset,y_trainset)


y_pred=classif_pipeline.predict(X_trainset)


from sklearn.metrics import accuracy_score
accuracy=accuracy_score(y_trainset,y_pred)
print("the accuracy score: ",accuracy)


from sklearn.metrics import classification_report, confusion_matrix
confusion_mrx=confusion_matrix(y_trainset,y_pred)

print(classification_report(y_trainset, y_pred))
confusion_mrx


#y_test_pred=classif_pipeline.predict(X_testset)
#evaluate on test set



from sklearn.metrics import precision_score,recall_score
percision=precision_score(y_trainset,y_pred)
recall=recall_score(y_trainset,y_pred)
print("Precision score: ",percision)
print("Recall score: ",recall)


#
from sklearn .linear_model import LogisticRegression
logistic_reg_pipeline=Pipeline(steps=[("preprocessor",preprocessor),
                                 ("logistic_reg_model",LogisticRegression())])


logistic_reg_pipeline.fit(X_trainset,y_trainset)
y_logistic_pred=logistic_reg_pipeline.predict(X_trainset)


accuracy_score(y_trainset,y_logistic_pred)


confusion_mtrx=confusion_matrix(y_trainset,y_logistic_pred)
confusion_mtrx


#randomforestclassifier on the testset
y_test_pred=classif_pipeline.predict(X_testset)



confusion_mrx_test=confusion_matrix(y_testset,y_test_pred)

print(classification_report(y_testset, y_test_pred))
confusion_mrx_test


#logitic regression on the testset
y_logist_pred=logistic_reg_pipeline.predict(X_testset)



#measure the perfomance of the logistic regression
confusion_mrx_log=confusion_matrix(y_testset,y_logist_pred)

print(classification_report(y_testset, y_logist_pred))
confusion_mrx_log


from joblib import dump
dump(classif_pipeline, 'at_risk_classifier_model.joblib')
















